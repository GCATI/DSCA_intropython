{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "    <div class=\"column\">\n",
    "        <img src=\"https://datasciencecampus.ons.gov.uk/wp-content/uploads/sites/10/2017/03/data-science-campus-logo-new.svg\"\n",
    "             alt=\"Data Science Campus Logo\"\n",
    "             align=\"right\" \n",
    "             width = \"340\"\n",
    "             style=\"margin: 0px 60px\"\n",
    "             />\n",
    "    </div>\n",
    "    <div class=\"column\">\n",
    "        <img src=\"https://cdn.ons.gov.uk/assets/images/ons-logo/v2/ons-logo.svg\"\n",
    "             alt=\"ONS Logo\"\n",
    "             align=\"left\" \n",
    "             width = \"420\"\n",
    "             style=\"margin: 0px 30px\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python: Day 2\n",
    "\n",
    "## Trainers\n",
    "\n",
    "<font size=\"+0.5\">Jhai Ghaghada</font>   \n",
    "(<jhai.ghaghada@ons.gov.uk>)  \n",
    "<font size=\"+0.5\">Daniel Lewis</font>   \n",
    "(<daniel.j.lewis@ons.gov.uk>)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<a href=\"#Background\"><font size=\"+1\">Background</font></a>\n",
    "* Learning Objectives\n",
    "* Setting Your Working Directory\n",
    "\n",
    "<a href=\"#Getting-Started\"><font size=\"+1\">Getting Started</font></a>\n",
    "* Importing pandas, reading data.\n",
    "* Reminder\n",
    "\n",
    "<a href=\"#Selecting-Columns-from-DataFrames\"><font size=\"+1\">Selecting Columns from DataFrames</font></a>\n",
    "* Selecting single and multiple columns.\n",
    "* <a href=\"#Exercise-1\">Exercise 1</a>\n",
    "\n",
    "<a href=\"#Filtering-rows-from-Dataframes\"><font size=\"+1\">Filtering Rows from DataFrames</font></a>\n",
    "* Simple filtering\n",
    "* Conditional filtering\n",
    "* <a href=\"#Exercise-2\">Exercise 2</a>\n",
    "* Using multiple conditions to filter\n",
    "* <a href=\"#Exercise-3\">Exercise 3</a>\n",
    "\n",
    "<a href=\"#Generating-New-Variables\"><font size=\"+1\">Generating New Variables</font></a>\n",
    "* Creating Binary Variables\n",
    "* Constant Value Variables\n",
    "* Creating Variables Based on Existing Columns\n",
    "* Classifying Numerical Values using pd.cut() and pd.qcut()\n",
    "* Removing (Dropping) Columns\n",
    "* <a href=\"#Exercise-4\">Exercise 4</a>\n",
    "\n",
    "<a href=\"#Descriptive-Statistics\"><font size=\"+1\">Descriptive Statistics</font></a>\n",
    "* Describing numerical data\n",
    "* Descriptive statistics for numerical data\n",
    "* Describing text (or categorical) data\n",
    "* <a href=\"#Exercise-5\">Exercise 5</a>\n",
    "\n",
    "<a href=\"#Updating-Values\"><font size=\"+1\">Updating Values</font></a>\n",
    "* Copies and Views\n",
    "* Selecting and Filtering with `.loc[row_indexer, col_indexer]` and `.iloc[row_indices, col_indices]`\n",
    "* Updating DataFrame Cells\n",
    "* Propagating Missing Data Values with .loc[]\n",
    "* Changing column data types\n",
    "* Changing column names\n",
    "\n",
    "<a href=\"#Aggregation\"><font size=\"+1\">Aggregation</font></a>\n",
    "* Aggregation using the `.groupby()` method.\n",
    "\n",
    "<a href=\"#Crosstabs,-Contingency,-and-Two-Way-Tables\"><font size=\"+1\">Crosstabs, Contingency, and Two Way Tables</font></a>\n",
    "* Crosstabulation using the `pd.crosstab()` top-level function.\n",
    "* <a href=\"#Exercise-6\">Exercise 6</a>\n",
    "\n",
    "<a href=\"#Merging-Data\"><font size=\"+1\">Merging Data</font></a>\n",
    "* Indexical Data\n",
    "* The `.merge()` function\n",
    "* Types of Merge\n",
    "* <a href=\"#Exercise-7\">Exercise 7</a>\n",
    "\n",
    "<a href=\"#Extra-Python\"><font size=\"+1\">Extra Python</font></a>\n",
    "* Iterable objects and `for` loops\n",
    "* While loops\n",
    "* Conditional statements.\n",
    "\n",
    "<a href=\"#Consolidation\"><font size=\"+1\">Consolidation</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Today we're going to establish a working knowledge of pandas DataFrames (and Series), including how to:\n",
    "* Select columns\n",
    "* Filter rows\n",
    "* Generate new variables\n",
    "* Describe numeric and categorical data\n",
    "* Update values and columns\n",
    "* Aggregate and cross-tabulate data\n",
    "* Merge\n",
    "\n",
    "## Setting Your Working Directory\n",
    "\n",
    "Remember from the last session that you can check your working directory using `%pwd` and change it using `%cd`.\n",
    "\n",
    "In the code cell below check that your working directory is the 'Notebooks' folder within the Introduction to Python Folder, if it is not, change the directory to Notebooks.\n",
    "\n",
    "While it is not essential for you to set the working directory, not doing so will mean that you'll have to alter all the relative paths for data and solutions set up in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and change your working directory here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In the last session, we learnt the code to import a python library. Use this to import pandas.\n",
    "\n",
    "We also learnt the code to read in some data into python using pandas. Read in the titanic csv dataset, and assign the DataFrame to a variable named `titanic`.\n",
    "\n",
    "Finally, check the first 5 rows of the DataFrame by calling the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here. Add additional cells if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a solution, in case you need help.\n",
    "%load ../Solutions/Intro/import_and_read.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting and Filtering DataFrames\n",
    "\n",
    "Over the following sections, we will learn how to select and filter data using pandas DataFrames. This is one of the most useful and powerful features of pandas.  \n",
    "\n",
    "It is useful for a range of reasons, from simply cutting down a large dataset into the specific sub-sets of data required for analysis, to managing the various components of a model (e.g. dependent and independent variables, training and test data etc.) and conducting specific subgroup analyses.\n",
    "\n",
    "Selecting and filtering can be done by using the indexing operator. Pandas uses the same indexing operator as lists, tuples, and dictionaries - `[]` (square brackets).\n",
    "\n",
    "However, the DataFrame indexing operator is more sophisticated than one used for the python built-in data structures, the behaviour of the DataFrame indexer depends, as you'll see, on what you pass to the DataFrame indexing operator. This allows you to pass different kinds of information to the same indexing operator and get specific outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Columns from DataFrames\n",
    "\n",
    "The simplest way to select a column from a dataframe is to use the name of that column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select by passing the name of a column as a string.\n",
    "passengers = titanic['name']\n",
    "passengers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the following things happen:\n",
    "1. We index the DataFrame `titanic` using the column header.\n",
    "2. Assuming 'name' is a legitimate column header, a pandas Series is returned.\n",
    "3. This Series, representing the 'name' column of the `titanic` Dataframe, is assigned to a variable called `passengers`\n",
    "5. Finally, we look at the first 5 rows of the Series object `passengers`\n",
    "\n",
    "If we want to select multiple columns, we have to first collect the column names together using a list, and then pass that to the DataFrame indexing operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make a list of column names called cols\n",
    "cols = ['name','age']\n",
    "# Use cols to select multiple columns from marvel.\n",
    "passenger_cols = titanic[cols]\n",
    "passenger_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is very similar to the code for selecting a single column, the main difference is that to select multiple columns we pass a list of string objects, rather that a single string object directly.\n",
    "\n",
    "However, because we have selected multiple columns, the `passenger_cols` object is actually a DataFrame, rather than a Series object.\n",
    "\n",
    "Note, that we don't have to create the `cols` list first, we can actually create it on-the-fly in the indexing operator, you just have to learn to distinguish the list constructor square brackets from the indexing square brackets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select multiple columns directly.\n",
    "passenger_cols = titanic[['name','age']]\n",
    "passenger_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just have a look, rather than assigning to a variable\n",
    "titanic[['name','age']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "1. Refresh your memory of the titanic data by getting:\n",
    "    * The number of rows and columns in the DataFrame\n",
    "    * The datatypes of the columns.\n",
    "    * A list of the columns names for the titanic dataset.\n",
    "2. Select the 'fare' column from the `titanic` data and show the tail of the data.\n",
    "3. Select just the last column, try using the list of column names you made earlier.\n",
    "4. Select the second, third and fourth columns, try doing it using DataFrame columns property directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these code cells to write your answers. Add more if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible solutions to the exercise.\n",
    "%load ../Solutions/Intro/exercise1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering rows from Dataframes\n",
    "\n",
    "Filtering rows from a pandas dataframe works in a very similar way to selecting columns. Simple filtering can be achieved by passing a range to the DataFrame indexer, just like slicing a list.\n",
    "\n",
    "## Simple Filtering\n",
    "\n",
    "The code below does the same thing as `.head()` and `.tail()` and can be used to show any arbitrary range of rows in a given DataFrame.\n",
    "\n",
    "Note that this is identical to slicing a list.\n",
    "\n",
    "However, we can't get individual rows by indexing as we would with a list, because a column could be named with an integer. This would mean that `dataframe[0]` is ambiguous and could refer to the first row, or a column named 0. Hence it is not allowed, `dataframe[0]` only works if you have a column named '0', which is a default for some operations in pandas.\n",
    "\n",
    "This means that selecting a single row also requires a slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 rows\n",
    "titanic[0:5] # or - titanic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 5 rows\n",
    "titanic[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary slice\n",
    "titanic[102:109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 row\n",
    "titanic[123:124]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Filtering\n",
    "\n",
    "Most filtering is a more involved operation where we first specify the condition(s) that must be met in order for rows to be included in or excluded from the output dataframe.\n",
    "\n",
    "## The Filtering Process\n",
    "\n",
    "The way that pandas filters rows can be though of as a two-step process.\n",
    "\n",
    "1. Create a 'mask' that specifies inclusion and exclusion for each row in the dataframe.\n",
    "2. Mask the dataframe to return the subset of rows that are included.\n",
    "\n",
    "This sounds a bit abstract, so let's consider what this might look like in practice.\n",
    "\n",
    "Imagine you have the following (very simple) dataframe, called 'catdog':\n",
    "\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "2 | Dog | Chewbarka\n",
    "3 | Cat | JK Meowling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to filter so you just have 'Cat' rows. Therefore you design the following condition:\n",
    "\n",
    "```python \n",
    "mask = catdog['Animal'] == 'Cat'\n",
    "```\n",
    "\n",
    "There are a lot of = (equals) in the above statement.\n",
    "* The first = indicates assignment, we are assigning the outcome of the expression on the right to the variable on the left of the equals sign.\n",
    "* The second double equals sign, ==, indicates a comparison, in this case it assesses whether each value in the 'Animal' column of catdog is equal to the text 'Cat'. If python finds that the column value and 'Cat' are the same it assigns a True value, and if not a False value.\n",
    "\n",
    "This produces a 'mask' which is a Series of `True` and `False` values against the DataFrame index.\n",
    "\n",
    "index | &#xfeff;\n",
    "---|---\n",
    "0 | True\n",
    "1 | True\n",
    "2 | False\n",
    "3 | True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you just have to pass the mask to the original dataframe to complete the filtering process.\n",
    "\n",
    "```python\n",
    "catdog2 = catdog[mask]\n",
    "catdog2\n",
    "```\n",
    "This subsets the catdog dataframe based on the True (include) and False (exclude) values. Producing:\n",
    "\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "3 | Cat | JK Meowling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row that had a 'Dog' value for 'Animal', has been removed. Note though that the index has remained the same as the original. Sometimes it is important to reset the index after filtering to restore the index to sequential integers starting at 0.\n",
    "\n",
    "If you want to reset the index - you can do so by using this code - \n",
    "\n",
    "```python\n",
    "catdog2 = catdog2.reset_index(drop = True)\n",
    "catdog2\n",
    "```\n",
    "index | Animal | Name\n",
    "---| --- | ---\n",
    "0 | Cat | Catalie Portman\n",
    "1 | Cat | Pico de Gato\n",
    "2 | Cat | JK Meowling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above that the index has been reset to be sequential.\n",
    "\n",
    "## Filtering Data\n",
    "\n",
    "We can filter by using logical comparison statements\n",
    "\n",
    "* == 'is equal to' notice the double == and watch out! A single one would be assigning to a variable!\n",
    "* != 'does not equal' - the opposite of ==\n",
    "* $\\gt$  greater than\n",
    "* $\\lt$ less than\n",
    "* $\\gt$= greater than or equal too\n",
    "* $\\lt$= less than or equal too.\n",
    "\n",
    "In addition, pandas includes some functions to make particular comparisons easier:\n",
    "\n",
    "* .isin(list) which we can use for multiple conditions \n",
    "* .between() which we can use to specify upper and lower bounds\n",
    "\n",
    "Finally, the ~ (tilde) allows us to flip or invert an expression. Basically, if an expression returns [true, true, false], the same expression with a ~ in front of it will return [false, false, true].\n",
    "\n",
    "However, we'll concentrate on the simple operators in the top list for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter titanic for 3rd class passengers only\n",
    "\n",
    "# First make the mask\n",
    "mask = titanic['pclass'] == 3\n",
    "# Have a quick look at the mask\n",
    "mask.sample(5) # 5 rows in the mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter the titanic dataframe with this mask\n",
    "thirdclass = titanic[mask]\n",
    "thirdclass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same approach for other logical statements.\n",
    "mask = titanic['fare'] > 200\n",
    "titanic[mask].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "1. Show the row for the passenger named: 'Birkeland, Mr. Hans Martin Monsen'\n",
    "2. How many passengers in the dataset are male?\n",
    "3. How many passengers are under 18 years of age?\n",
    "4. What proportion of passenger in the dataset survived?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here, continue by adding new code cells is you wish.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise 2 solutions\n",
    "%load ../Solutions/Intro/Exercise2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Multiple Conditions to Filter\n",
    "\n",
    "So far, we've only filtered according to individual conditions set on a single column, but there is no reason we can't use multiple conditions to filter by several conditions and/or columns at once. However, we do need to think about how the conditions relate to each other, we have two options to establish these relationships.\n",
    "\n",
    "* **and** relationships are given by the **&** (ampersand) symbol. This implies both/all conditions must be met for a row to evaluate to True.\n",
    "* **or** relationships are given by the **|** (pipe) symbol. This implies that if _any_ of the conditions can be met a given row evaluates to True.\n",
    "\n",
    "You can think of `.isin()` and `.between()` as being special versions of multiple condition filters.\n",
    "\n",
    "* isin() is basically just a lot of linked **or** statements - *value1* **or** *value2* **or** *value3* etc.\n",
    "* between() is an **and** condition - greater than (or equal to) the lower bound **and** less than (or equal to) the upper bound.\n",
    "\n",
    "Let's again take a simple example to illustrate this with the `catdog` dataframe:\n",
    "\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "0 | Cat | Catalie Portman | 3.0\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "2 | Dog | Chewbarka | 1.0\n",
    "3 | Cat | JK Meowling | 7.0\n",
    "4 | Dog | K-9 | 11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to select all animals that are cats **and** who are over 4 years old, you could do the following:\n",
    "\n",
    "```python\n",
    "mask = (catdog['Animal'] == 'Cat') & (catdog['Age'] > 4.0)\n",
    "\n",
    "catdog[mask]\n",
    "```\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "3 | Cat | JK Meowling | 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Cats over 4 years old have been included in the filter.\n",
    "\n",
    "However, if you wanted to select all animals that are either cats **or** are over 4 years old, you could instead do:\n",
    "\n",
    "```python\n",
    "mask = (catdog['Animal'] == 'Cat') | (catdog['Age'] > 4.0)\n",
    "\n",
    "catdog[mask]\n",
    "```\n",
    "index | Animal | Name | Age\n",
    "---| --- | --- | ---\n",
    "0 | Cat | Catalie Portman | 3.0\n",
    "1 | Cat | Pico de Gato | 5.0\n",
    "3 | Cat | JK Meowling | 7.0\n",
    "4 | Dog | K-9 | 11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try some multiple condition filters with the titanic data\n",
    "# First class passengers who are women.\n",
    "mask = (titanic['pclass'] == 1) & (titanic['sex']== 'female')\n",
    "titanic[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Women or children\n",
    "mask = (titanic['sex'] == 'female') | (titanic['age'] < 18)\n",
    "titanic[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the special functions for multiple selection. First .isin()\n",
    "# Passeners from Cherbourg ('C') or Queenstown ('Q')\n",
    "titanic[titanic['embarked'].isin(['C','Q'])].sample(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, .between()\n",
    "# passengers who paid between 100 and 250\n",
    "titanic[titanic['fare'].between(100,250)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "1. Select passengers who are in classes 2 and 3, what percentage of passengers is this?\n",
    "2. How many passengers who do not have siblings or spouses ('sibsp'), or parents or children ('parch') on the boat?\n",
    "3. What proportion of passengers who 'embarked' in Cherbourg ('C') or Queenstown ('Q') survived? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here, add cells as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 solutions\n",
    "%load ../Solutions/Intro/exercise3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New Variables\n",
    "\n",
    "There are a number of approaches to adding new columns of data to a DataFrame, and depending on what you want to achieve some of these can be quite complicated. We start with simple examples and build up to more sophisticated approaches later.\n",
    "\n",
    "## Creating Binary Variables\n",
    "\n",
    "We can assign a condition to a new column to create a binary variable in much the same way as we might create a mask for filtering rows.\n",
    "\n",
    "Imagine that instead of filtering rows, we instead wanted to assign a 1 or a 0 to a new column depending on whether that condition was met. We can do this quite simply due to the fact that in python (and many other languages) a `True` value is equivalent to 1, and a `False` value is equivalent to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the condition as a True/False Series\n",
    "cond = titanic['sex'] == 'female'\n",
    "\n",
    "# Now assign the condition variable cond to a new column, but as an integer type.\n",
    "titanic['female'] = cond.astype(int)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, a condition is specified, returing a boolean Series. This Series object is converted to an integer data type and assigned to a new column in the `titanic` DataFrame called 'female'. If 'female' already existed, this code would have overwritten whatever was already in runtime.\n",
    "\n",
    "Using a condition we can either store `True` and `False` values directly, or convert them to their integer representations `1` and `0`. If we wanted to use arbitrary values for our new variable we can create a dictionary and `.map()` the dictionary to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use YES and NO instead of True and False.\n",
    "titanic['female'] = cond.map({True:'YES',False:'NO'})\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this `.map()` approach is also good for aggregating (dissolving) or recoding categorical variables. The dictionary can be of an arbitrary length and act as a lookup, this is a benefit of the key:value structure. Note though that pandas also implements its own `category` variable. We're not going to discuss it here as it's not strictly necessary, but it can be useful. Check the [pandas docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Value Variables\n",
    "\n",
    "Constant values can also be assigned to all rows in a DataFrame with a single number or string, with the data type being dictated by the format of the value being assigned. This may be useful in the context of updating cells, which we'll discuss later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column of ints\n",
    "titanic['int_zeroes'] = 0\n",
    "# New column of floats\n",
    "titanic['float_ones'] = 1.0 # note floating point.\n",
    "# New column of strings\n",
    "titanic['string_twos'] = 'two'\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Variables Based on Existing Columns\n",
    "\n",
    "We can simply assign the values of existing columns to new columns using assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['name2'] = titanic['name']\n",
    "titanic.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use mathematical expressions with one or more existing columns to create new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The + 1 constant accounts for the passenger themself.\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "titanic.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Numerical Values using pd.cut() and pd.qcut()\n",
    "\n",
    "New columns can be created directly when classifying numerical data using `pd.cut()` or `pd.qcut()`\n",
    "\n",
    "`cut()` has two behaviours. The default is to create a given number of equal-sized bins (e.g. the width of all bins are the same), however if you provide bins it will cut according to those bins.\n",
    "\n",
    "`qcut()` is similar, but rather than equal-sized bins it created bins that have (roughly) equal numbers of observations in them. These bins could have very different widths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide fare into 3 equally sized classes.\n",
    "titanic['fare_3class'] = pd.cut(titanic['fare'], 3, labels=['low','mid','high'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide fare into tertile.\n",
    "titanic['fare_tertiles'] = pd.qcut(titanic['fare'], 3, labels=['low','mid','high'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing (Dropping) Columns\n",
    "\n",
    "Not withstanding the fact that we could select the specific columns we want and exclude ones we don't want in our dataset, we also have a couple of options for dropping or deleting a column from a pandas DataFrame.\n",
    "\n",
    "Let's delete the constant valued columns we established earlier: 'int_zeroes', 'float_ones', 'string_twos'.\n",
    "\n",
    "The first option we have is the built in python statement `del`. Otherwise we can use the `.drop()` method.\n",
    "\n",
    "Note, that pandas is in active development, so sometimes parameters change as the library matures. If your `.drop()` method doesn't understand the code below then it may be an older version. try this instead:\n",
    "```python\n",
    "titanic.drop(['int_zeroes','float_ones','string_twos'], axis=1, inplace=True)\n",
    "```\n",
    "Note that this syntax is still compatible with newer versions of pandas, so old code won't break in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop a column with del\n",
    "del titanic['int_zeroes']\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop using the drop method\n",
    "titanic.drop(columns=['float_ones','string_twos'], inplace = True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "1. Create a new binary variable called 'child', it should have the value 1 when the passenger is under 18 and 0 otherwise.\n",
    "2. Create a new variable called 'embarked_city', map 'S' to 'Southampton', 'C' to 'Cherbourg', and 'Q' to 'Queenstown'.\n",
    "3. Create a new variable called 'surname', the value should be the surname part of the 'name' field.\n",
    "    * Use `titanic['name'].str.split(',',expand=True)[0]` to get surnames.\n",
    "    * Explore this code and make sure you understand what's going on.\n",
    "    * How many unique surnames are there (hint: try `.unique()` or `.nunique()` on the new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 solutions\n",
    "%load ../Solutions/Intro/exercise4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that last question we made use of a special module on the `Series` object called `str`, this exposes all the string methods we've encountered previously to a column of string data, but in a vectorised form. This means you can manipulate text in a row-by-row manner with a single method call. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case names\n",
    "titanic['name'].str.lower().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press tab after the last fullstop to see the string methods available.\n",
    "# Select one and use shift-tab to explore it further.\n",
    "pd.Series.str."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example we are trying to extract all the titles from the names field.\n",
    "titanic['name'].str.split(',',expand=True)[1].str.split('.',expand=True)[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "\n",
    "There are really two broad types of data in our DataFrames at the moment that we want to look at - numerical data (i.e. ints and floats) and text data (i.e. strings; slightly confusingly called objects).\n",
    "\n",
    "In this section, we will explore some basic univariate descriptive statistics.\n",
    "\n",
    "## Describing Numerical Data\n",
    "Let's start with the numerical data, because thats the easiest to work with. Pandas even has a built in function called `.describe()` which will provide some descriptive statistics for all the numerical columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the titanic dataframe\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, the descriptive statistics output for our titanic dataset is also a DataFrame!\n",
    "\n",
    "Make sure you understand what each row means in this table:\n",
    "* **count** - the number (count) of entries in the given column.\n",
    "* **mean** - the average (arithmetic mean) data value in the given column.\n",
    "* **std** - the standard deviation (spread) of values in the given column.\n",
    "* **min** - the smallest value in the given column.\n",
    "* **25%** - the value of the data at the lower quartile (i.e. after the first 25% of data, ordered from smallest to largest).\n",
    "* **50%** - the middle value of the data (aka the median), half the values are larger than this value, and half smaller.\n",
    "* **75%** - the value of the data at the upper quartile (i.e. after the first 75% of data, ordered from smallest to largest).\n",
    "* **max** - the maximum data value recorded.\n",
    "\n",
    "We can get a sense of the data from these descriptive statistics. For instance: \n",
    "\n",
    "## Descriptive Statistics for Numerical Data\n",
    "\n",
    "`.describe()` is great to get an overview, but what if we just wanted particular statistics and not the whole lot?\n",
    "\n",
    "Well, pandas will let you run a range of statistics individually! Some examples are given in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count() can be defined for all datatypes, so all columns are computed. Note which columns have some missing data.\n",
    "titanic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean() is only defined for numeric columns\n",
    "titanic.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std() is also only defined for numeric columns\n",
    "titanic.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min() has a definition for numeric and text data.\n",
    "# The minimum value of a text field is the text which is first alphabetically.\n",
    "titanic.min() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max() has a definition for numeric and text data.\n",
    "# The maximum value of a text field is the text which is last alphabetically.\n",
    "titanic.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile() allows you to specify quantiles, such as 0.25 (lower quartile), 0.5 (median), and 0.75 (upper quartile)\n",
    "# for convenience median() also exists\n",
    "titanic.quantile(0.25) # 25% - lower quartile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum() works to concatenate text, producing a curious output.\n",
    "titanic.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopefully though it is obvious that these methods could be called on selected columns too.\n",
    "titanic['fare'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As it happens, python has a built-in sum, min and max functions which does the same thing.\n",
    "# however, pandas sum is better when confronted with missing data:\n",
    "sum(titanic['fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this instead\n",
    "sum(titanic[titanic['fare'].notnull()]['fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, a new filter condition for working with missing data is apparent: `notnull()` this returns `True` for rows that have a valid value, and `False` otherwise. Similar to the behaviour of `bool()`. The opposite of `notnull()` is `isnull()`.\n",
    "\n",
    "This method of selection is similar to making conditional statements with object methods that return a Boolean, e.g.\n",
    "```python\n",
    "if string_variable.islower():\n",
    "    # Do something\n",
    "```\n",
    "The same principle can apply to other contexts, for instance the `Series` object has a large number of string methods collected as `.str.`, calling `titanic['name'].str.contains('Mr.', regex=False)` returns `True` or `False` for each row in a column depending on whether it contains the substring 'Mr.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select passengers with title Mr. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mr.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select passengers with title Mrs. and get mean fare\n",
    "titanic[titanic['name'].str.contains('Mrs.', regex=False)]['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing Text (or Categorical) Data\n",
    "\n",
    "We can still use `.describe()` to look at text data, however we need to specify that we're looking at object (text) data types.\n",
    "\n",
    "Really, the descriptive statistics below are for categorical data, they don't work very well if every value in a field is a different piece of text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the describe parameters we're only choosing to include object datatypes, given by 'O'.\n",
    "# The 'O' is in a list, because we could include other data types in the list if we wanted to.\n",
    "titanic.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are describing an object you get some different summary statistics than with numerical data:\n",
    "\n",
    "* **count** as before, a count of the values present in each column.\n",
    "* **unique** a count of the number of unique values in each column.\n",
    "* **top** is the most common value - aka the mode.\n",
    "* **freq** is the frequency of occurance of the most common value.\n",
    "\n",
    "Let's dig a bit deeper into some of these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interestingly there are 2 James Kellys, however they don't appear to be duplicates.\n",
    "titanic[titanic['name'] == 'Kelly, Mr. James']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way we could check for other name duplicates is by taking the mode.\n",
    "# As mode can be non-unique it returns a series\n",
    "# Looks like Kate Connolly is another possible duplicate.\n",
    "titanic['name'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, these appear to be different people!\n",
    "titanic[titanic['name'] == 'Connolly, Miss. Kate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the unique() function will give us all the unique objects in a column.\n",
    "titanic['embarked_city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the value_counts() function gives a count for each unique value in a chosen column.\n",
    "titanic['embarked_city'].value_counts()\n",
    "# Most people embarked in Southampton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Data\n",
    "\n",
    "Sorting data is straightforward in pandas, a simple sort on one columns used the DataFrame method `.sort_values()`:\n",
    "```python\n",
    "titanic.sort_values('age')\n",
    "```\n",
    "The default is to sort in ascending order, from smallest to largest value. Set the ascending parameter to `False` for a descending sort:\n",
    "```python\n",
    "titanic.sort_values('age', ascending = False)\n",
    "```\n",
    "This approach sorts and returns the entire DataFrame, if you want to sort a single column on its works similarly:\n",
    "```python\n",
    "titanic['age'].sort_values()\n",
    "```\n",
    "More complicated sorting behaviours can be managed by passing a list, in the order you would like the sort to occur:\n",
    "```python\n",
    "titanic.sort_values(['pclass','age'], ascending = [True, False])\n",
    "```\n",
    "In the above code I sort first by 'pclass' then by 'age'. in addition I pass a list to ascending indicating that 'pclass' is to be sorted in ascending order, and 'age' in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort fare descending\n",
    "titanic.sort_values('fare', ascending = False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by sex ascending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = [True, False]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by sex descending, then age descending\n",
    "titanic.sort_values(['sex','age'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "\n",
    "1. How old is the oldest passenger in the dataset?\n",
    "2. How many men and women are in the dataset?\n",
    "    * Check the pd.Series.value_counts() docstring and figure out how to get proportions of men and women.\n",
    "3. Create a new column called 'std_fare' which is the 'fare' minus the mean fare, divided by the standard deviation.\n",
    "4. Calculate the number of children in second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions for exercise 5\n",
    "%load ../Solutions/Intro/exercise5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating Values\n",
    "\n",
    "\n",
    "## Important! Copies and Views\n",
    "\n",
    "The selecting of columns and filtering of rows that you've done above effectively **copies** the original dataframe to make a new one based on your conditions. This is great for creating a pared-down dataframe to work with, or when filtering data to produce statistics for particular subsets of data.\n",
    "\n",
    "However, when we want to actually update cell values we need to use a special function that ensures we are editing the original dataframe **view** and not a **copy** of it. If we edit cells on a copy of a dataframe, we might find that these values are not updated in the original dataframe when we come to analyse it!\n",
    "\n",
    "This is a technical point that you don't need to worry too much about, just remember to use the following approaches when dealing with code!\n",
    "\n",
    "In the titanic data, given what we know so far, we might try to update a cell values using what's called 'chained indexing':\n",
    "\n",
    "```python\n",
    "titanic[titanic['embarked'] == 'C']['embarked'] = 'c'\n",
    "```\n",
    "The above code makes sense - filter the rows, select the column and assign the value you want. However, python will give you a warning (specifically a 'SettingWithCopyWarning') that you're not doing things properly!\n",
    "\n",
    "Instead we'll do the following:\n",
    "\n",
    "```python\n",
    "titanic.loc[titanic['embarked'] == 'C','embarked'] = 'c'\n",
    "```\n",
    "\n",
    "The code looks very similar, however in the second (correct) example we're using `.loc[]`\n",
    "\n",
    "`.loc[]` is actually almost identical to the selecting and filtering we've already done. We just specify the rows and columns within the square bracket like: `.loc[row_indexer, col_indexer]` where:\n",
    "* `row_indexer` is the mask if we are filtering, or a colon, `:`, if we want to include all rows.\n",
    "* `col_indexer` is a single column name, or a list of column names, or a colon, `:`, if we want to include all columns.\n",
    "\n",
    "You can actually use the .loc[] approach to do all the selecting and filtering we've already done if you want.\n",
    "\n",
    "In addition to `.loc[]` we also have `.iloc[]` which behaves very similarly, except that it allows us to index on the index position of rows and columns.\n",
    "\n",
    "Some examples using iloc[]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows, and the first column\n",
    "titanic.iloc[:,0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or select a range of columns\n",
    "titanic.iloc[:,3:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another head-like indexing approach\n",
    "titanic.iloc[0:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some examples using `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Often researchers use column selection to reorder the columns in a DataFrame.\n",
    "cols = list(titanic.columns)\n",
    "cols.reverse()\n",
    "titanic.loc[:,cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc takes row conditions as usual.\n",
    "titanic.loc[titanic['pclass'].isin([1,2]),['pclass','name']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often pandas users will use chained indexing when selecting and filtering, and the `loc` and `iloc` operators when cleaning values on a cell-by-cell basis. There is no reason why you can't use these operators for selecting and filtering too, however, if you want to assign the sub-set DataFrame to a new variable you'll need to use the `.copy()` method to ensure that it is a different DataFrame in memory, rather than a reference to the original.\n",
    "```python\n",
    "new_df = titanic.loc[:,['survived','embarked']].copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagating Missing Data Values with `.loc[]`\n",
    "\n",
    "Earlier, when we created a new binary variable called 'child' the condition we used created a `Series` of `True` and `False` values based on whether the condition was met or not.\n",
    "\n",
    "An important consideration when creating a new variable is presence of missing data. A condition will automatically set a missing value to `False`. This is fine for filtering data, but misrepresents data in a new variable - it makes a derived variable appear more complete than it is in reality.\n",
    "\n",
    "In order to update the 'child' column to incorporate rows we know are missing we can use `.loc[]`:\n",
    "```python\n",
    "# This was the column originally generated.\n",
    "titanic['child'] = (titanic['age'] < 18).astype(int)\n",
    "# Now, update the missing data values\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "```\n",
    "In the 2nd line of code, we use the row filter `titanic['age'].isnull()` to filter missing data in the 'age' column and we select the 'child' column. These are used to create a 'view' of the titanic DataFrame using `.loc[]`.\n",
    "\n",
    "Then, we assign to that combination of rows and columns the values given by: `titanic[titanic['age'].isnull()]['age']` which will be a Series of missing values. We should now see missing values in the 'child' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial composition of 'child'\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update and check new field\n",
    "titanic.loc[titanic['age'].isnull(),'child'] = titanic[titanic['age'].isnull()]['age']\n",
    "titanic['child'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Column Data Types\n",
    "\n",
    "As with basic python data types, we can cast columns of data from one data type to another. This can be useful as part of a data cleaning process. Sometimes we find that data we expect to be numeric is actually string data, often occurs because the numbers are actually stored as text in the original dataset, in which case they have to be converted. Other times it is because the original dataset includes characters that pandas won't immediately interpret as a numeric value. This is particularly the case if a dataset contains missing data that is coded to a special character. Reading in these data as text is the safest option, as it preserves all of the information and requires the data analyst to make a decision as to how to handle the conversion to a numeric data type.\n",
    "\n",
    "There are two ways to change a datatype, firstly the Series method: `.astype()` in which the parameter is a data type e.g.\n",
    "```python\n",
    "titanic['survived'].astype(str)\n",
    "```\n",
    "There is also a 'top-level' pandas function `pd.to_numeric()` which is a good option for data cleaning.\n",
    "```python\n",
    "pd.to_numeric(titanic['survived'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turned survived to a string column\n",
    "titanic['survived'] = titanic['survived'].astype(str)\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turned survived back to a numeric\n",
    "titanic['survived'] = pd.to_numeric(titanic['survived'])\n",
    "titanic.dtypes['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Column Names\n",
    "\n",
    "We've been using a dataset that has good, descriptive column names. However, sometimes you're faced with columns that have difficult to use names.\n",
    "\n",
    "* column names might be really long, these a are a pain to type out.\n",
    "* spaces between words, and presence of special characters (e.g. %^&£ etc.) can be annoying\n",
    "* columns names may be ambiguous or not sufficiently descriptive.\n",
    "\n",
    "We can use the pandas `.rename()` method to update column names. To update the column names we pass a dicitonary to the parameter 'columns', e.g.\n",
    "```python\n",
    "df.rename(columns={'two':'new_name'}, inplace=True)\n",
    "```\n",
    "For a DataFrame `df`, the `rename` method allows us to change the names of columns based upon a dictionary. Here, the dictionary key `'two'` is the current name of the column, and the dictionary value `'new_name'` is what we want to rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.rename(columns={'surname':'family_name'}, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "Aggregation means grouping data together by a particular grouping variable and producing a summary of one or more columns for that grouping variable.\n",
    "\n",
    "We'll use the `groupby()` function. \n",
    "\n",
    "This function can be really useful, especially when your data are disaggregate - e.g. data about individual units of people or things. \n",
    "\n",
    "`groupby()` allows you to aggregate by a categorical variable and summarise numerical data into a new dataframe.\n",
    "\n",
    "`.groupby()` works on a principle known as 'split-apply-combine':\n",
    "* Split - a dataframe is divided into a set of smaller dataframes based on the grouping variable.\n",
    "* Apply - an aggregation is applied to each of the groups to create a single row for each group in the original dataframe.\n",
    "* Combine - bring together the aggregated dataframe rows into a final new dataframe.\n",
    "\n",
    "Let's walk through what that might look like for the `titanic` dataframe:\n",
    "* Firstly, we decide to **split** the data by the 'pclass'. This divides the `titanic` dataframe into effectively three separate dataframes, one for first, one for second and one for third class.\n",
    "* Secondly, we **apply** an aggregation to the dataframe. You can either produce an aggregate statistic for all rows, or you can selected specific columns on which to do the aggregation. If we **apply** a `.mean()` aggregation to 'fare', then for each 'pclass' group we get the average fare cost.\n",
    "* Finally, pandas returns a **combined** dataframe that contains the new aggregate statistics.\n",
    "\n",
    "Let's look at that in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('pclass')['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly a count of children by class might look like this:\n",
    "titanic.groupby('pclass')['child'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this all sounds fairly straightforward! `.groupby()` is a powerful tool, particularly when you are working with any kind of hierarchical data where you might want to know something aggregate about the groups within the data, for instance:\n",
    "* individuals nested in households.\n",
    "* employees nested in firms.\n",
    "* patients nested in primary or secondary care trusts.\n",
    "* small area geographies (e.g. wards, output areas, postcodes etc.) nested in larger geographies (e.g. districts, counties etc.)\n",
    "* countries nested in supra-national entities.\n",
    "\n",
    "or, demographic, cultural and socio-economic classes:\n",
    "* individuals by age, sex, ethnicity, religion etc.\n",
    "* employees by grade or occupational social class.\n",
    "* households by neighbourhood deprivation rank or decile.\n",
    "* experimental subjects in intervention and control arms of a trial.\n",
    "\n",
    "We can also aggregate according to more complicated groupings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby passenger class, then city of embarkation.\n",
    "titanic.groupby(['pclass','embarked_city'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB order is important to the output.\n",
    "titanic.groupby(['embarked_city','pclass'])['fare'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordering of groups may be important as it affects the resultant DataFrame.\n",
    "\n",
    "If you assign the `groupby()` output to a variable, you can also pull out dataframes for particular groups, just as if you had written a filter condition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = titanic.groupby('pclass')\n",
    "classes.get_group(3).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstabs, Contingency, and Two-Way Tables\n",
    "\n",
    "Similar to aggregation with `.groupby()` a cross-tabulation table allows you to generate frequencies for combinations of groups of data.\n",
    "\n",
    "Crosstabs are often also refered to as 'contingency tables' and 'two-way tables' and, although there may be some subtle distinctions, these all refer to the setting of one categorical variable against another, creating a matrix, and then counting, or otherwise aggregating by cell values.\n",
    "\n",
    "To create cross-tabulations, you can use the `pandas.crosstab()` function.\n",
    "\n",
    "Let's have a look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic use of `.crosstab()` requires that you input the row category ('survived') and the column category ('sex'). This produces cell counts for the cross-tabulation of the two categories, so there were 339 passengers who were both 'female' and 'survived'.\n",
    "\n",
    "The basic crosstab can be augmented with some additional parameters:\n",
    "* `margins` - set to `True` to add row and column totals.\n",
    "* `normalize` - create row or column proportions, or overall proportions, rather than frequencies. Keywords are 'index', 'columns' or 'all'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crosstab of survived against sex\n",
    "pd.crosstab(titanic['survived'],titanic['sex'], margins=True, normalize = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "\n",
    "1. What is the average fare paid by men and women?\n",
    "2. What is the median fare paid by men and women in each different class?\n",
    "3. Create a crosstab for 'survived' and 'pclass', which class is the best for survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 solutions\n",
    "%load ../Solutions/Intro/exercise6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data\n",
    "\n",
    "Often all the data you need to answer a question are not contained within a single dataset, but across several. Datasets can be joined, or 'merged', to allow data to be analysed together, but only **if the two datasets share a common reference or identifier.**\n",
    "\n",
    "Linking data come in a number of forms, and are commonly refered to as 'indexical' data. Some examples include:\n",
    "\n",
    "* Your NHS number, allowing data linkage across the NHS for primary, secondary, tertiary care episodes and prescribing.\n",
    "* Any account number (e.g. banking, utilities, travel card, council tax etc.) can acts as a point of linkage between different sets of data.\n",
    "* Your email, phone number, social media handles etc.\n",
    "* Your address can also act as a spatial reference, linking you to your neighbourhood, local services etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different ways to merge\n",
    "\n",
    "While you may have heard it called \"Join\" in other languages, particularly in database query languages, in pandas we use the `.merge()` function.\n",
    "\n",
    "Once you have established that two DataFrames share a reference that will permit a merge to be conducted, you may wish to further specify how the merge behaves with the `how` parameter.\n",
    "\n",
    "* Inner - Only rows with reference values that appear in both DataFrames are merged.\n",
    "* Left - All the data from the 'left' DataFrame is retained, and any rows that have matching references are merged from the 'right' DataFrame.\n",
    "* Right - All data from the right and anything that matches from the left. Effectively, the reverse of 'Left'.\n",
    "* Outer (Full) - all data from the left and right DataFrame is retained, matched up where possible.\n",
    "\n",
    "This can be easier to understand graphically:\n",
    "\n",
    "![joins](https://www.dofactory.com/Images/sql-joins.png)\n",
    "\n",
    "Let’s read in some additional titanic data and have a look at them.\n",
    "\n",
    "The new dataset includes the passenger name and age, as well as the additional variables:\n",
    "* boat - lifeboat identifier\n",
    "* body - body identification number\n",
    "* home.dest - the passenger's home and destination in the form \"home / dest\" or just \"home\".\n",
    "\n",
    "The dataset is located in the 'Data' folder, it is an excel file called: 'titanic_more.xlsx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the titanic_more.xlsx using pandas.\n",
    "titanic_more = pd.read_excel('../Data/titanic_more.xlsx')\n",
    "titanic_more.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge the two tables we need to use a column which uniquely defines each passenger and is available in both DataFrames. At first glance, 'name' would appear to be a good candidate for this, however, remember there are a couple of passengers who have the same name as each other.\n",
    "\n",
    "We can explicitly check is a column uniquely identifies rows with the `Series.is_unique` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic\n",
    "titanic['name'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name is not unique defined for each row in titanic_more\n",
    "titanic_more['name'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can create a field that will uniquely identify passengers in both datesets by generating a new variable that combines the 'name' and 'age' variables. This is because we happen to know the ages of the passengers who have the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic['name_age_id'] = titanic['name'] + \" \" + titanic['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic['name_age_id'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique id base don name and age for titanic\n",
    "titanic_more['nameageid'] = titanic_more['name'] + \" \" + titanic_more['age'].astype(str)\n",
    "# Check if the new variable is unique\n",
    "titanic_more['nameageid'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have unique id fields in both titanic and titanic_more, we use them to merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the titanic_more dataset with titanic\n",
    "titanic_merge = titanic.merge(titanic_more[['name_age_id','boat','body','home.dest']], on = 'name_age_id')\n",
    "titanic_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell above we merge the `titanic_more` DataFrame into `titanic`. We do this on the basis of the 'name_age_id' variable that we created.\n",
    "\n",
    "The default merge behaviour is 'inner' join, however in this particular case all behaviours ('inner', 'left', 'right','outer') resolve to the same outcome as both datasets include the same 1,309 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that pandas actually handles multiple columns directly for unique identification.\n",
    "titanic_merge = titanic.merge(titanic_more[['name','age','boat','body','home.dest']], on = ['name','age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "\n",
    "Load the revised dataset 'titanic_revised.xlsx' which only includes passengers which have a value for boat, body or home.dest and try merging this to titanic.\n",
    "\n",
    "1. Which types of merge perform as expected?\n",
    "    * Try the different `how` parameters: 'inner', 'outer', 'left', 'right'.\n",
    "2. How many values are observed for boat, body, and home.dest?\n",
    "    * i.e. How many values are non-missing/\n",
    "    * Hint: use `.count()`\n",
    "3. How many passengers in the dataset used lifeboat number 3? What proportion of them were female?\n",
    "    * Hint: think about datatypes.\n",
    "4. How many passengers record 'New York' or 'NY' somewhere in the 'home.dest' column?\n",
    "    * If you do a selection using `Series.str.contains()` you need to specify the parameter na=False.\n",
    "    * This sets missing values to `False` in the boolean filter and excludes them from the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 solution\n",
    "%load ../Solutions/Intro/exercise7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Python\n",
    "\n",
    "What follows are a few additional things from python that you might except to have been told about. We'll look at these only if we have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidation\n",
    "\n",
    "## Reminder of Learning Objectives\n",
    "\n",
    "Today we're going to establish a working knowledge of pandas DataFrames (and Series), including how to:\n",
    "* Describe numeric and categorical data\n",
    "* Update values and columns\n",
    "* Aggregate and cross-tabulate data\n",
    "* Merge data\n",
    "\n",
    "## Vignettes\n",
    "\n",
    "There are two additional notebooks to explore in the notebooks folder.\n",
    "1. [Vignette - Visualisation](Vignette-Visualisation.ipynb) : explores the basics of making graphical plots using pandas objects and the matplotlib plotting library.\n",
    "2. [Vignette - Statistics](Vignette-Statistics.ipynb): explores some basic statistics, including linear regression using pandas objects and the statsmodels econometric statistics package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
